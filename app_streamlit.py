import streamlit as st
import os
from dotenv import load_dotenv
import nest_asyncio
import re
from PIL import Image
from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.dml.color import RGBColor

from PIL import Image as PILImage
from pptx.enum.text import PP_ALIGN
import time
import pandas as pd


nest_asyncio.apply()
from llama_cloud_services import LlamaParse
from langchain_text_splitters import MarkdownHeaderTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
import openai
import json
from scripts.clova_ocr_utils import ocr_image
import requests

# .env í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="PDF Chat App", layout="wide")


def find_header_y_from_ocr(image_path, header_text, api_url, secret_key):
    """
    CLOVA OCR ê²°ê³¼ì—ì„œ header_textì™€ ê°€ì¥ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ì˜ yì¢Œí‘œ ë°˜í™˜
    """
    ocr_result = ocr_image(image_path, api_url, secret_key)
    best_y = None
    best_score = 0
    for field in ocr_result["images"][0]["fields"]:
        text = field["inferText"]
        bbox = field["boundingPoly"]["vertices"]
        if header_text in text:
            y_top = min(v["y"] for v in bbox)
            return y_top, text, bbox
    return None, None, None


def clova_ocr_multipart(image_path, api_url, secret_key):
    """
    CLOVA OCR General APIë¥¼ multipart/form-data ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ
    :param image_path: ì—…ë¡œë“œí•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
    :param api_url: CLOVA OCR API Gateway Invoke URL
    :param secret_key: CLOVA OCR Secret Key
    :return: OCR ê²°ê³¼(JSON)
    """
    headers = {"X-OCR-SECRET": secret_key}

    files = {
        "file": open(image_path, "rb"),
        "message": (
            None,
            '{"version": "V2", "requestId": "1234", "timestamp": 1722225600000, "lang": "ko", "images": [{"format": "jpg", "name": "page_7.jpg"}]}',
        ),
    }

    response = requests.post(
        api_url,
        headers=headers,
        files=files,
    )
    response.raise_for_status()
    return response.json()


def normalize_text(text):
    # í•œê¸€ë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì œê±° (ìˆ«ì, ì˜ì–´, íŠ¹ìˆ˜ë¬¸ì, ë„ì–´ì“°ê¸° ëª¨ë‘ ì œê±°)
    return re.sub(r"[^ê°€-í£]", "", text)


def scale_ocr_y_to_image_y(ocr_y, ocr_result, image_path):
    img = Image.open(image_path)
    img_width, img_height = img.size
    ocr_height = ocr_result["images"][0]["convertedImageInfo"]["height"]
    return (ocr_y / ocr_height) * img_height


def extract_header_y_from_ocr_response(
    ocr_response, header_text, x_gap_threshold=40, image_path=None
):
    fields = ocr_response["images"][0]["fields"]
    lines = []
    current_line = []
    prev_field = None
    for field in fields:
        if not current_line:
            current_line.append(field)
        else:
            prev_right_x = max(v["x"] for v in prev_field["boundingPoly"]["vertices"])
            curr_left_x = min(v["x"] for v in field["boundingPoly"]["vertices"])
            if curr_left_x - prev_right_x > x_gap_threshold:
                lines.append(current_line)
                current_line = [field]
            else:
                current_line.append(field)
        prev_field = field
        if field.get("lineBreak", False):
            lines.append(current_line)
            current_line = []
            prev_field = None
    if current_line:
        lines.append(current_line)

    norm_header = normalize_text(header_text)
    # === ì˜ˆì™¸ ì²˜ë¦¬: 'ì•”ì§„ë‹¨' -> 'ì‚¼ì§„ë‹¨', 'ì•”ì¹˜ë£Œ' -> 'ì°¸ì¹˜ë£Œ' ===
    ocr_compare_header = norm_header
    if norm_header == "ì•”ì§„ë‹¨":
        ocr_compare_header = "ì‚¼ì§„ë‹¨"
    elif norm_header == "ì•”ì¹˜ë£Œ":
        ocr_compare_header = "ì°¸ì¹˜ë£Œ"
    # =====================================================
    strict_headers = {"ìˆ˜ìˆ ", "ì…ì›", "ì¬í•´"}
    for line_fields in lines:
        line_text = "".join(f["inferText"] for f in line_fields)
        norm_line = normalize_text(line_text)
        if norm_header in strict_headers:
            # ì˜ˆì™¸: ì™„ì „ ì¼ì¹˜ë§Œ í—ˆìš©
            if norm_line == norm_header:
                all_y = [
                    v["y"] for f in line_fields for v in f["boundingPoly"]["vertices"]
                ]
                y_top = min(all_y)
                bbox_list = [f["boundingPoly"]["vertices"] for f in line_fields]
                if image_path is not None:
                    y_top_scaled = scale_ocr_y_to_image_y(
                        y_top, ocr_response, image_path
                    )
                    return y_top_scaled, line_text, bbox_list
                else:
                    return y_top, line_text, bbox_list
        else:
            # ì¼ë°˜: ë¶€ë¶„ ë¬¸ìì—´ + ë¼ì¸ ê¸¸ì´ ì œí•œ
            if (
                ocr_compare_header in norm_line
                and len(norm_line) <= len(ocr_compare_header) * 1.5
            ):
                all_y = [
                    v["y"] for f in line_fields for v in f["boundingPoly"]["vertices"]
                ]
                y_top = min(all_y)
                bbox_list = [f["boundingPoly"]["vertices"] for f in line_fields]
                if image_path is not None:
                    y_top_scaled = scale_ocr_y_to_image_y(
                        y_top, ocr_response, image_path
                    )
                    return y_top_scaled, line_text, bbox_list
                else:
                    return y_top, line_text, bbox_list
    return None, None, None


def crop_image_by_y(image_path, y_start, y_end, output_path):
    img = Image.open(image_path)
    img_width, img_height = img.size
    y_start = int(max(0, y_start - 15))
    y_end = int(min(img_height, y_end))
    x_start = max(0, 30)
    x_end = min(img_width, img_width - 30)
    if y_end <= y_start or x_end <= x_start:
        return None  # ë¹ˆ ì´ë¯¸ì§€ ë°©ì§€
    cropped = img.crop((x_start, y_start, x_end, y_end))
    cropped.save(output_path)
    return output_path


def extract_headers_from_llamaparse_items(items, page_number):
    """LlamaParse itemsì—ì„œ header(page, level, text) ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (object íƒ€ì… ëŒ€ì‘)"""
    headers = []
    for item in items:
        if getattr(item, "type", None) == "heading":
            headers.append(
                {
                    "page": page_number,
                    "level": getattr(item, "lvl", 1),
                    "text": getattr(item, "value", ""),
                    "page_number": page_number,
                }
            )
    return headers


def build_header_dictionary(headers, ocr_results_dict, output_dir="temp_output"):
    """
    headers: [{page, level, text}, ...]
    ocr_results_dict: {page_number: ocr_result, ...}
    return: {"header_text": {page, level, text, y, crop_image_path, Header 1, Header 2, Header 3}}
    """
    header_dict = {}
    from collections import defaultdict

    # í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”
    page_to_headers = defaultdict(list)
    for header in headers:
        if header["y"] is not None:
            page_to_headers[header["page"]].append(header)

    # ê° í˜ì´ì§€ë³„ë¡œ Header 1, 2, 3 ì •ë³´ ìˆ˜ì§‘
    page_headers = defaultdict(lambda: {"Header 1": "", "Header 2": "", "Header 3": ""})
    for page_number, page_headers_list in page_to_headers.items():
        # yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_headers = sorted(page_headers_list, key=lambda x: x["y"])

        # ê° ë ˆë²¨ë³„ë¡œ ê°€ì¥ ìœ„ì— ìˆëŠ” í—¤ë”ë¥¼ ì°¾ìŒ
        for header in sorted_headers:
            level = header["level"]
            if level == 1 and not page_headers[page_number]["Header 1"]:
                page_headers[page_number]["Header 1"] = header["text"]
            elif level == 2 and not page_headers[page_number]["Header 2"]:
                page_headers[page_number]["Header 2"] = header["text"]
            elif level == 3 and not page_headers[page_number]["Header 3"]:
                page_headers[page_number]["Header 3"] = header["text"]

    # ê° í˜ì´ì§€ì˜ í—¤ë”ë³„ë¡œ ì´ë¯¸ì§€ ìƒì„±
    for page_number, page_headers_list in page_to_headers.items():
        page_img_path = os.path.join(output_dir, f"page_{page_number}.jpg")
        img = Image.open(page_img_path)
        img_height = img.size[1]

        # levelë³„ë¡œ ê·¸ë£¹í™”
        level_to_headers = defaultdict(list)
        for h in page_headers_list:
            level_to_headers[h["level"]].append(h)

        for level, headers_in_level in level_to_headers.items():
            headers_in_level = sorted(headers_in_level, key=lambda h: h["y"])
            for i, header in enumerate(headers_in_level):
                y_start = header["y"]
                if i < len(headers_in_level) - 1:
                    y_end = headers_in_level[i + 1]["y"]
                else:
                    y_end = img_height
                if y_end <= y_start:
                    continue

                out_path = os.path.join(
                    output_dir,
                    f"page_{page_number}_level{level}_header_{i+1}_{header['text'][:10]}.jpg",
                )
                crop_image_by_y(page_img_path, y_start, y_end, out_path)

                key = f"{header['text']}"
                header_dict[key] = {
                    "page": page_number,
                    "level": level,
                    "text": header["text"],
                    "y": y_start,
                    "crop_image_path": out_path,
                    "Header 1": page_headers[page_number]["Header 1"],
                    "Header 2": page_headers[page_number]["Header 2"],
                    "Header 3": page_headers[page_number]["Header 3"],
                    "page_number": page_number,
                }

    return header_dict


def add_source_text(slide, header_info, prs, pdf_name=""):
    """í•˜ë‹¨ ì¤‘ì•™ì— ì¶œì²˜ í…ìŠ¤íŠ¸ ì¶”ê°€"""
    # page_numberë¥¼ header_infoì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° (page ë˜ëŠ” page_number í‚¤ ì‚¬ìš©)
    page_number = header_info.get("page_number") or header_info.get("page", "?")

    # ì¶œì²˜ í…ìŠ¤íŠ¸ ë°•ìŠ¤
    width = Inches(6)  # ê³ ì •ëœ ë„ˆë¹„
    left = (prs.slide_width - width) / 2  # ì¤‘ì•™ ì •ë ¬
    top = prs.slide_height - Inches(1.2)  # AI ê³ ì§€ ë¬¸êµ¬ ìœ„ì— ìœ„ì¹˜
    height = Inches(0.5)

    ref_box = slide.shapes.add_textbox(left, top, width, height)
    ref_tf = ref_box.text_frame
    ref_tf.word_wrap = True

    p_ref = ref_tf.paragraphs[0]
    p_ref.alignment = PP_ALIGN.CENTER

    run1 = p_ref.add_run()
    run1.text = "[ì¶œì²˜: "
    run1.font.size = Pt(12)
    run1.font.color.rgb = RGBColor(120, 120, 120)

    run2 = p_ref.add_run()
    run2.text = f'"{pdf_name}", {page_number}page]'
    run2.font.size = Pt(12)
    run2.font.color.rgb = RGBColor(120, 120, 120)


def add_header_path(slide, header_info, prs):
    """ì œëª© ìœ„ì— ê³„ì¸µì  í—¤ë” ê²½ë¡œ ì¶”ê°€"""
    # levelê³¼ textë¥¼ ì‚¬ìš©í•´ ê³„ì¸µì  í—¤ë” êµ¬ì„±
    level = header_info.get("level", 0)
    text = header_info.get("text", "")

    # í˜„ì¬ í—¤ë”ì˜ ìƒìœ„ í—¤ë”ë“¤ì„ ì°¾ê¸° ìœ„í•´ pageì™€ yì¢Œí‘œ ì‚¬ìš©
    page = header_info.get("page", 0)
    y = header_info.get("y", 0)

    # ê°™ì€ í˜ì´ì§€ì˜ ëª¨ë“  í—¤ë”ë¥¼ ê°€ì ¸ì™€ì„œ yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    all_headers = []
    for key, info in st.session_state.get("header_dict", {}).items():
        if info.get("page") == page:
            all_headers.append(info)

    # yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    all_headers.sort(key=lambda x: x.get("y", 0))

    # í˜„ì¬ í—¤ë”ë³´ë‹¤ ìœ„ì— ìˆëŠ” í—¤ë”ë“¤ ì¤‘ ì§ê³„ ìƒìœ„ í—¤ë”ë§Œ ì°¾ê¸°
    header_text = ""
    for h in all_headers:
        if h.get("y", 0) < y:
            h_level = h.get("level", 0)
            # í˜„ì¬ í—¤ë”ì˜ ì§ê³„ ìƒìœ„ í—¤ë”ë§Œ ì¶”ê°€
            if h_level == level - 1:
                if header_text:
                    header_text += " >> "
                header_text += h.get("text", "")

    # í˜„ì¬ í—¤ë” ì¶”ê°€
    if header_text:
        header_text += " >> " + text
    else:
        header_text = text

    # í—¤ë” ê²½ë¡œ í…ìŠ¤íŠ¸ ë°•ìŠ¤
    left = Inches(0.3)
    top = Inches(0.4)  # ì œëª©ë³´ë‹¤ ë” ìœ„ì— ìœ„ì¹˜
    width = prs.slide_width - Inches(0.6)
    height = Inches(0.5)

    path_box = slide.shapes.add_textbox(left, top, width, height)
    path_tf = path_box.text_frame
    path_tf.word_wrap = True

    p_path = path_tf.paragraphs[0]
    p_path.text = header_text
    p_path.font.size = Pt(12)  # 12ptë¡œ ë³€ê²½
    p_path.font.bold = True
    p_path.font.color.rgb = RGBColor(0, 0, 255)  # íŒŒë€ìƒ‰
    p_path.alignment = PP_ALIGN.LEFT


def add_title_text(slide, header_info, prs):
    """ìƒë‹¨ì— í° ì œëª© ì¶”ê°€"""
    # ê³„ì¸µì  í—¤ë” ê²½ë¡œì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    header_path = header_info.get("text", "")
    if " >> " in header_path:
        title_text = header_path.split(" >> ")[-1]
    else:
        title_text = header_path

    title_left = Inches(0.3)
    title_top = Inches(0.6)
    title_width = prs.slide_width - Inches(0.6)
    title_height = Inches(1)

    title_box = slide.shapes.add_textbox(
        title_left, title_top, title_width, title_height
    )
    title_tf = title_box.text_frame
    title_tf.word_wrap = True

    p_title = title_tf.paragraphs[0]
    p_title.text = title_text
    p_title.font.size = Pt(24)
    p_title.font.bold = True
    p_title.font.color.rgb = RGBColor(80, 80, 80)
    p_title.alignment = PP_ALIGN.LEFT


def add_ai_notice_text(slide, prs):
    """í•˜ë‹¨ ì¤‘ì•™ì— ìƒì„±í˜• AI ê³ ì§€ ë¬¸êµ¬ ì¶”ê°€"""
    bottom_text = (
        "ë³¸ ìë£ŒëŠ” ìƒì„±í˜• AI ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ì¤‘ìš”í•œ ì‚¬ì‹¤ì€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
    )

    box_width = Inches(6)
    bottom_left = (prs.slide_width - box_width) / 2
    bottom_top = prs.slide_height - Inches(0.7)
    bottom_height = Inches(0.5)

    ai_box = slide.shapes.add_textbox(bottom_left, bottom_top, box_width, bottom_height)
    ai_tf = ai_box.text_frame
    ai_tf.word_wrap = True

    p_ai = ai_tf.paragraphs[0]
    p_ai.text = bottom_text
    p_ai.font.size = Pt(12)
    p_ai.font.color.rgb = RGBColor(150, 150, 150)
    p_ai.alignment = PP_ALIGN.CENTER


def add_center_image(slide, header_info, prs):
    """ì¤‘ì•™ì— ì´ë¯¸ì§€ ì¶”ê°€"""
    img_path = header_info.get("crop_image_path")
    if img_path and os.path.exists(img_path):
        with PILImage.open(img_path) as im:
            img_width, img_height = im.size
            slide_width = prs.slide_width
            slide_height = prs.slide_height
            slide_ratio = slide_width / slide_height
            img_ratio = img_width / img_height

            if img_ratio > slide_ratio:
                width = slide_width * 0.85
                height = width / img_ratio
            else:
                height = slide_height * 0.65
                width = height * img_ratio

        left = int((slide_width - width) / 2)
        top = int((slide_height - height) / 2)
        slide.shapes.add_picture(img_path, left, top, int(width), int(height))


def create_ppt_from_header_dict(
    header_list, output_pptx, pdf_name="êµë³´ë§ˆì´í”Œëœê±´ê°•ë³´í—˜[2409](ë¬´ë°°ë‹¹)"
):
    prs = Presentation()
    blank_slide_layout = prs.slide_layouts[6]

    # ì²« ë²ˆì§¸ ìŠ¬ë¼ì´ë“œì— í‘œì§€ ì¶”ê°€
    cover_slide = prs.slides.add_slide(blank_slide_layout)

    # êµë³´ ë¡œê³  ì´ë¯¸ì§€ ì¶”ê°€
    logo_path = os.path.join("temp_output", "kyobo_logo.jpg")
    if os.path.exists(logo_path):
        # ì´ë¯¸ì§€ í¬ê¸° ê³„ì‚° (ê°€ë¡œê°€ ìŠ¬ë¼ì´ë“œì— ê½‰ ì°¨ë„ë¡)
        with PILImage.open(logo_path) as im:
            img_width, img_height = im.size
            slide_width = prs.slide_width
            slide_height = prs.slide_height

            # ê°€ë¡œê°€ ìŠ¬ë¼ì´ë“œì— ê½‰ ì°¨ë„ë¡ ë¹„ìœ¨ ê³„ì‚°
            width = slide_width
            height = (img_height / img_width) * width

            # ì„¸ë¡œê°€ ìŠ¬ë¼ì´ë“œë³´ë‹¤ í¬ë©´ ì„¸ë¡œ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ê³„ì‚°
            if height > slide_height:
                height = slide_height
                width = (img_width / img_height) * height

            # ì´ë¯¸ì§€ë¥¼ ì¤‘ì•™ì— ë°°ì¹˜
            left = int((slide_width - width) / 2)
            top = int((slide_height - height) / 2)

            cover_slide.shapes.add_picture(
                logo_path, left, top, int(width), int(height)
            )

    # PDF ì´ë¦„ê³¼ ë‚ ì§œ í…ìŠ¤íŠ¸ ì¶”ê°€
    from datetime import datetime

    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

    # PDF ì´ë¦„ í…ìŠ¤íŠ¸ ë°•ìŠ¤
    pdf_text_left = Inches(0.3)
    pdf_text_top = Inches(5)  # ë¡œê³  ì•„ë˜ì— ìœ„ì¹˜
    pdf_text_width = prs.slide_width - Inches(0.6)
    pdf_text_height = Inches(0.5)

    pdf_text_box = cover_slide.shapes.add_textbox(
        pdf_text_left, pdf_text_top, pdf_text_width, pdf_text_height
    )
    pdf_text_tf = pdf_text_box.text_frame
    pdf_text_tf.word_wrap = True

    p_pdf = pdf_text_tf.paragraphs[0]
    p_pdf.text = f'í˜„ì¬ PDFì´ë¦„: "{pdf_name}"'
    p_pdf.font.size = Pt(14)
    p_pdf.font.color.rgb = RGBColor(150, 150, 150)
    p_pdf.alignment = PP_ALIGN.CENTER

    # ë‚ ì§œ í…ìŠ¤íŠ¸ ë°•ìŠ¤
    date_text_left = Inches(0.3)
    date_text_top = Inches(5.5)  # PDF ì´ë¦„ ì•„ë˜ì— ìœ„ì¹˜
    date_text_width = prs.slide_width - Inches(0.6)
    date_text_height = Inches(0.5)

    date_text_box = cover_slide.shapes.add_textbox(
        date_text_left, date_text_top, date_text_width, date_text_height
    )
    date_text_tf = date_text_box.text_frame
    date_text_tf.word_wrap = True

    p_date = date_text_tf.paragraphs[0]
    p_date.text = today
    p_date.font.size = Pt(12)
    p_date.font.color.rgb = RGBColor(150, 150, 150)
    p_date.alignment = PP_ALIGN.CENTER

    # ë‚˜ë¨¸ì§€ ìŠ¬ë¼ì´ë“œ ìƒì„±
    for header_info in header_list:
        # Header 1 ë ˆë²¨ì˜ í—¤ë”ëŠ” ê±´ë„ˆë›°ê¸°
        if header_info.get("level") == 1:
            continue

        slide = prs.slides.add_slide(blank_slide_layout)

        # 1. ì¤‘ì•™ì— ì´ë¯¸ì§€ ì¶”ê°€
        add_center_image(slide, header_info, prs)

        # 2. ì¶œì²˜ í…ìŠ¤íŠ¸ ì¶”ê°€
        add_source_text(slide, header_info, prs, pdf_name)

        # 3. í—¤ë” ê²½ë¡œ ì¶”ê°€
        add_header_path(slide, header_info, prs)

        # 4. ì œëª© í…ìŠ¤íŠ¸ ì¶”ê°€
        add_title_text(slide, header_info, prs)

        # 5. AI ê³ ì§€ ë¬¸êµ¬ ì¶”ê°€
        add_ai_notice_text(slide, prs)

    prs.save(output_pptx)


def get_deepest_header_and_level(metadata):
    for depth in reversed(range(1, 5)):  # Header4ê¹Œì§€ í™•ì¥ ê°€ëŠ¥
        key = f"Header {depth}"
        if key in metadata and metadata[key]:
            return metadata[key], depth
    return None, None


# 2ë‹¨ ì»¬ëŸ¼ ë ˆì´ì•„ì›ƒ (ì™¼ìª½: ì±„íŒ…, ì˜¤ë¥¸ìª½: íŒŒì¼ ì—…ë¡œë“œ)
col_chat, col_upload = st.columns([2, 1])

# FAISS DBë¥¼ ì„¸ì…˜ì— ì €ì¥
if "faiss_db" not in st.session_state:
    st.session_state["faiss_db"] = None

with col_chat:
    st.header("ğŸ’¬ êµìœ¡ ìë£Œ ìƒì„±í•˜ê¸°")
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []
    # ì±„íŒ… ë‚´ì—­ í‘œì‹œ
    for msg in st.session_state["chat_history"]:
        st.markdown(f"**{msg['role']}**: {msg['content']}")
    # ì‚¬ìš©ì ì…ë ¥
    user_input = st.text_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="user_input")

    # ê²€ìƒ‰ ë²„íŠ¼ ë° ê²°ê³¼ ì¶œë ¥
    if st.button("ê²€ìƒ‰"):
        if st.session_state["faiss_db"] is not None and user_input:
            header_dict = st.session_state.get("header_dict", {})
            retriever = st.session_state["faiss_db"].as_retriever(
                search_type="similarity", search_kwargs={"k": 3}
            )
            results = retriever.invoke(user_input)
            st.subheader("ê²€ìƒ‰ ê²°ê³¼")
            if isinstance(results, list):
                for i, doc in enumerate(results):
                    page = doc.metadata.get("page_number", "ì•Œ ìˆ˜ ì—†ìŒ")
                    st.markdown(f"**ê²°ê³¼ {i+1} (í˜ì´ì§€: {page})**")
                    st.code(doc.page_content)
                    # === ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸° ì¶”ê°€ ===
                    header_text, level = get_deepest_header_and_level(doc.metadata)
                    matched_header = None
                    for h in header_dict.values():
                        if (
                            h["page"] == page
                            and h["level"] == level
                            and h["text"] == header_text
                        ):
                            matched_header = h
                            break
                    if (
                        matched_header
                        and matched_header.get("crop_image_path")
                        and os.path.exists(matched_header["crop_image_path"])
                    ):
                        st.image(
                            matched_header["crop_image_path"],
                            caption=f"ì´ë¯¸ì§€ ({header_text})",
                            use_column_width=True,
                        )
            else:
                doc = results
                page = doc.metadata.get("page_number", "ì•Œ ìˆ˜ ì—†ìŒ")
                st.markdown(f"**ê²°ê³¼ (í˜ì´ì§€: {page})**")
                st.code(doc.page_content)
                st.json(doc.metadata)
                header_text, level = get_deepest_header_and_level(doc.metadata)
                matched_header = None
                for h in header_dict.values():
                    if (
                        h["page"] == page
                        and h["level"] == level
                        and h["text"] == header_text
                    ):
                        matched_header = h
                        break
                if (
                    matched_header
                    and matched_header.get("crop_image_path")
                    and os.path.exists(matched_header["crop_image_path"])
                ):
                    st.image(
                        matched_header["crop_image_path"],
                        caption=f"Crop ì´ë¯¸ì§€ (í—¤ë”: {header_text})",
                        use_column_width=True,
                    )
            # === ê²€ìƒ‰ ê²°ê³¼ PPT ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ===
            selected_headers = []

            if isinstance(results, list):
                for doc in results:
                    page = doc.metadata.get("page_number")
                    header_text, level = get_deepest_header_and_level(doc.metadata)
                    for h in header_dict.values():
                        if (
                            h["page"] == page
                            and h["level"] == level
                            and h["text"] == header_text
                        ):
                            selected_headers.append(h)
            else:
                doc = results
                page = doc.metadata.get("page_number")
                header_text, level = get_deepest_header_and_level(doc.metadata)
                for h in header_dict.values():
                    if (
                        h["page"] == page
                        and h["level"] == level
                        and h["text"] == header_text
                    ):
                        selected_headers.append(h)
            st.session_state["selected_headers"] = selected_headers
            # ê²€ìƒ‰ ì‹œì ì— PPTë¥¼ ë¯¸ë¦¬ ìƒì„±í•˜ê³  ê²½ë¡œë¥¼ ì„¸ì…˜ì— ì €ì¥
            pptx_path = os.path.join("temp_output", "search_results.pptx")

            if selected_headers:
                create_ppt_from_header_dict(selected_headers, pptx_path)
                st.session_state["search_pptx_path"] = pptx_path
            else:
                st.session_state["search_pptx_path"] = None

            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì€ ì„¸ì…˜ ê°’ë§Œ ì‚¬ìš©
            if st.session_state.get("search_pptx_path") and os.path.exists(
                st.session_state["search_pptx_path"]
            ):
                with open(st.session_state["search_pptx_path"], "rb") as f:
                    st.download_button(
                        label="ê²€ìƒ‰ ê²°ê³¼ PPT ë‹¤ìš´ë¡œë“œ",
                        data=f,
                        file_name="search_results.pptx",
                        mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                    )
            else:
                st.info("ê²€ìƒ‰ ê²°ê³¼ì— í•´ë‹¹í•˜ëŠ” í—¤ë” ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ë¨¼ì € ë²¡í„°ìŠ¤í† ì–´ì— ì„ë² ë”©ì„ ì €ì¥í•˜ê³ , ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

with col_upload:
    st.header("ğŸ“„ íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])
    if uploaded_file:
        st.info("íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ íŒŒì‹±ì„ ì‹œì‘í•˜ì„¸ìš”.")
        output_dir = "temp_output"
        os.makedirs(output_dir, exist_ok=True)
        md_save_path = os.path.join(
            output_dir, f"temp_{os.path.splitext(uploaded_file.name)[0]}.md"
        )
        pptx_path = os.path.join(output_dir, "exported_slides.pptx")
        progress_bar = st.progress(0, text="ëŒ€ê¸° ì¤‘...")
        status_text = st.empty()
        if "parsing_in_progress" not in st.session_state:
            st.session_state["parsing_in_progress"] = False
        if not st.session_state["parsing_in_progress"]:
            if ("pptx_path" in st.session_state and st.session_state["pptx_path"]) or (
                "md_save_path" in st.session_state and st.session_state["md_save_path"]
            ):
                st.success("íŒŒì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                pptx_path = st.session_state.get("pptx_path")
                md_save_path = st.session_state.get("md_save_path")
                if pptx_path:
                    try:
                        with open(pptx_path, "rb") as f:
                            st.download_button(
                                label="íŒŒì‹±ëœ PPT íŒŒì¼ ì „ì²´ ë‹¤ìš´ë¡œë“œ",
                                data=f,
                                file_name="exported_slides.pptx",
                                mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                            )
                    except Exception as e:
                        st.error(f"PPT íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                if md_save_path:
                    try:
                        with open(md_save_path, "rb") as f_md:
                            st.download_button(
                                label="íŒŒì‹±ëœ Markdown íŒŒì¼ ì „ì²´ ë‹¤ìš´ë¡œë“œ",
                                data=f_md,
                                file_name=os.path.basename(md_save_path),
                                mime="text/markdown",
                            )
                    except Exception as e:
                        st.error(f"Markdown íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            else:
                if st.button("íŒŒì‹± ì‹œì‘", type="primary"):
                    st.session_state["parsing_in_progress"] = True
                    st.experimental_rerun()
        if st.session_state["parsing_in_progress"]:
            status_text.info("ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë³€í™˜ì¤‘...")
            for fake_percent in range(5, 41, 5):
                progress_bar.progress(fake_percent, text=f"ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë³€í™˜ì¤‘...")
                time.sleep(0.5)
            temp_pdf_path = os.path.join(output_dir, f"temp_{uploaded_file.name}")
            with open(temp_pdf_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            parser_obj = LlamaParse(
                api_key=os.getenv("LLAMA_CLOUD_API_KEY"),
                num_workers=1,
                verbose=True,
                language="ko",
                extract_layout=True,
                premium_mode=True,
                continuous_mode=False,
                extract_charts=True,
                save_images=True,
                output_tables_as_HTML=False,
                max_pages=7,
            )
            # í˜ì´ì§€ ìˆ˜ ì˜ˆì¸¡ (ì—†ìœ¼ë©´ 7ë¡œ ê°€ì •)
            total_pages = 7
            result = parser_obj.parse(temp_pdf_path)
            md_docs = result.get_markdown_documents(split_by_page=True)
            st.session_state["md_docs"] = md_docs
            st.session_state["llama_parse_result"] = result
            progress_bar.progress(40, text="ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„± ì™„ë£Œ!")
            status_text.success("ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„± ì™„ë£Œ!")
            for idx, page in enumerate(result.pages):
                percent = 40 + int(10 * (idx + 1) / total_pages)
                progress_bar.progress(
                    percent, text=f"ë§ˆí¬ë‹¤ìš´ í›„ì²˜ë¦¬ì¤‘... ({idx+1}/{total_pages})"
                )
            for page in result.pages:
                images = getattr(page, "images", [])
                for img in images:
                    if getattr(img, "type", None) == "full_page_screenshot":
                        img_name = getattr(img, "name", None)
                        if img_name:
                            result.save_image(img_name, output_dir)
            if md_docs:
                with open(md_save_path, "w", encoding="utf-8") as f:
                    for doc in md_docs:
                        f.write(doc.text)
                st.session_state["md_save_path"] = md_save_path
                st.info(f"ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {md_save_path}")
            progress_bar.progress(50, text="ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ì¤‘...")
            status_text.info("ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ì¤‘...")
            md_save_path = st.session_state["md_save_path"]
            with open(md_save_path, "r", encoding="utf-8") as f:
                md_text = f.read()
            headers_to_split_on = [
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
            ]
            markdown_splitter = MarkdownHeaderTextSplitter(
                headers_to_split_on=headers_to_split_on
            )
            all_chunks = []
            md_docs = st.session_state.get("md_docs", [])
            for doc in md_docs:
                chunks = markdown_splitter.split_text(doc.text)
                for chunk in chunks:
                    chunk.metadata["page_number"] = doc.metadata.get("page_number")
                all_chunks.extend(chunks)
            progress_bar.progress(70, text="ì„ë² ë”© ê³„ì‚°ì¤‘...")
            embeddings = OpenAIEmbeddings()
            db = FAISS.from_documents(all_chunks, embeddings)
            st.session_state["faiss_db"] = db
            progress_bar.progress(80, text="OCR ë¡œ ì¢Œí‘œ ê³„ì‚°ì¤‘...")
            status_text.info("OCR ë¡œ ì¢Œí‘œ ê³„ì‚°ì¤‘...")
            result = st.session_state.get("llama_parse_result")
            api_url = "https://8vb79ndbzb.apigw.ntruss.com/custom/v1/41373/3a26df9469f0c22bb024a70d5cc5e11a9fb28f6ea21993ba8eee769f4dda9216/general"
            secret_key = os.getenv("CLOVA_OCR_SECRET_KEY")
            headers = []
            ocr_results_dict = {}
            for page_idx, page in enumerate(result.pages):
                page_number = getattr(page, "page", page_idx + 1)
                items = getattr(page, "items", [])
                page_headers = extract_headers_from_llamaparse_items(items, page_number)
                page_img_path = os.path.join(output_dir, f"page_{page_number}.jpg")
                ocr_results_dict[page_number] = clova_ocr_multipart(
                    page_img_path, api_url, secret_key
                )
                for header in page_headers:
                    ocr_result = ocr_results_dict.get(page_number)
                    y, matched_text, bbox_list = extract_header_y_from_ocr_response(
                        ocr_result, header["text"], image_path=page_img_path
                    )
                    header["y"] = y
                headers.extend(page_headers)
                percent = 80 + int(15 * (page_idx + 1) / total_pages)
                progress_bar.progress(
                    percent, text=f"OCR ë° crop ì§„í–‰ì¤‘... ({page_idx+1}/{total_pages})"
                )
            header_dict = build_header_dictionary(
                headers, ocr_results_dict, output_dir=output_dir
            )
            st.session_state["header_dict"] = header_dict
            # íŒŒì‹±ì´ ëë‚œ ì§í›„ ìµœì‹  header_dictë¡œ PPT ìƒì„±
            create_ppt_from_header_dict(list(header_dict.values()), pptx_path)
            # header_dictë¥¼ í‘œë¡œ ì‹œê°í™”
            df = pd.DataFrame(list(header_dict.values()))
            st.subheader("íŒŒì‹±ëœ í—¤ë”/ì´ë¯¸ì§€ ì •ë³´ í‘œ")
            st.dataframe(df)
            progress_bar.progress(100, text="ìƒì„± ì™„ë£Œ!")
            status_text.success("ëª¨ë“  íŒŒì¼ì´ íŒŒì‹±ëœ PPT ìƒì„± ì™„ë£Œ!")
            st.session_state["pptx_path"] = pptx_path
            st.session_state["parsing_in_progress"] = False
            # íŒŒì‹± ì§í›„ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ë°”ë¡œ ë…¸ì¶œ
            st.success("íŒŒì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
            pptx_path = st.session_state.get("pptx_path")
            md_save_path = st.session_state.get("md_save_path")
            if pptx_path:
                try:
                    with open(pptx_path, "rb") as f:
                        st.download_button(
                            label="íŒŒì‹±ëœ PPT íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                            data=f,
                            file_name="exported_slides.pptx",
                            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                        )
                except Exception as e:
                    st.error(f"PPT íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            if md_save_path:
                try:
                    with open(md_save_path, "rb") as f_md:
                        st.download_button(
                            label="íŒŒì‹±ëœ Markdown íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                            data=f_md,
                            file_name=os.path.basename(md_save_path),
                            mime="text/markdown",
                        )
                except Exception as e:
                    st.error(f"Markdown íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
